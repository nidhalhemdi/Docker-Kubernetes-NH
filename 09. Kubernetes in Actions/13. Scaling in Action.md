# **Kubernetes Scaling in Action**

This lecture demonstrates **manual scaling** of a Deployment and shows how Kubernetes distributes traffic across multiple Pods.

---

# 1Ô∏è‚É£ **Manual Scaling with `kubectl scale`**

If you **don‚Äôt have autoscaling** enabled, you can manually create more Pods using:

```
kubectl scale deployment/first-app --replicas=3
```

This tells Kubernetes:

* Keep **3 Pods** of the `first-app` Deployment running.
* Kubernetes **creates new Pods** until the desired number is reached.

---

# 2Ô∏è‚É£ **After Scaling**

Run:

```
kubectl get pods
```

You‚Äôll see:

* The original Pod
* Two additional Pods
* All running the same container image defined by the Deployment

This also appears in the Kubernetes dashboard.

---

# 3Ô∏è‚É£ **Traffic Distribution (Load Balancing)**

Because the service you created is a **load balancer** (NodePort in Minikube, or cluster-level load balancing):

* Incoming traffic is **distributed across Pods**
* You won‚Äôt always hit the same Pod on every request

### Demonstration:

* Visiting `/error` crashes one Pod (intentionally).
* The application **still works** because traffic is rerouted to the **other Pods** that are still running.
* Even if one Pod is restarting, traffic keeps flowing through the remaining Pods.

This is real-world resilience.

---

# 4Ô∏è‚É£ **Scaling Down**

You can reduce the number of Pods:

```
kubectl scale deployment/first-app --replicas=1
```

Kubernetes then:

* Terminates the extra Pods
* Leaves only one Pod running

During scale-down:

* You‚Äôll temporarily see Pods in **Terminating**
* Eventually only a single Pod remains

---

# üìå **Key Takeaways**

* **Scaling** = adjusting the number of Pods for a Deployment.
* Scaling can be **manual** (`kubectl scale`) or automated (via autoscalers ‚Äî covered later).
* A Service automatically **load balances** traffic across all Pods.
* If a Pod crashes, other Pods continue serving requests.
* Scaling down removes Pods until only the desired number remains.
