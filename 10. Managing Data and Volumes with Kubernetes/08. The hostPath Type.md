# â€œA Second Volume: The *hostPath* Typeâ€

After using `emptyDir`, we now show its limitation and introduce the **hostPath** volume type.

---

# ğŸš« Problem with `emptyDir`: It is *pod-specific*

With `emptyDir`:

* Each **Pod gets its own unique empty directory**.
* Even if you have **multiple replicas** of the same Deployment, each replica gets its **own separate `emptyDir` volume**.
* Therefore, **data is NOT shared** across Pod replicas.

### Example:

When replicas = 2:

```
Pod A â†’ emptyDir volume A
Pod B â†’ emptyDir volume B
```

If a request goes to Pod B:

* Pod B will NOT see data stored in Pod Aâ€™s volume.

This is exactly what happened:

* You wrote data in Pod A
* Pod A crashed, traffic went to Pod B
* Pod Bâ€™s volume is empty â†’ â€œfailed to open fileâ€

---

# ğŸ†• Why introduce **hostPath**?

Because `hostPath` lets **multiple Pods on the same node** share the **same directory on the machine (node)**.

Instead of attaching a pod-local folder:

```
Pod âŸ¶ emptyDir (local to pod)
```

We attach a directory from the nodeâ€™s filesystem:

```
Worker Node /host/path âŸ¶ Pod 1
                        âŸ¶ Pod 2
                        âŸ¶ Pod 3
```

Now all Pods running on the same node **see the same directory** â†’ they share the same data.

---

# ğŸ§± Step 1: Replace `emptyDir` with `hostPath`

Old:

```yaml
emptyDir: {}
```

New:

```yaml
hostPath:
  path: /data
  type: DirectoryOrCreate
```

### Meaning of the fields:

#### **`path:`**

Path **on the node** (the real machine) where data is stored.

This is similar to a **Docker bind mount**.

#### **`type:`**

* **Directory** â†’ must exist, otherwise Kubernetes errors
* **DirectoryOrCreate** â†’ if the directory does not exist, K8s creates it automatically

We need **DirectoryOrCreate** because `/data` doesnâ€™t exist yet on the node.

---

# ğŸ§± Step 2: Apply the Deployment

Kubernetes terminates the 2 old Pods and starts 2 new ones using the hostPath volume.

---

# ğŸ§ª Behavior with hostPath

### After writing data:

All Pods on the node see the same file:

```
Node: /data/text.txt
     â†‘           â†‘
 Pod 1         Pod 2
```

Now:

* Crash Pod 1 â†’ restarts
* Requests may go to Pod 2
* **Pod 2 still sees `/data/text.txt`**, because itâ€™s shared from the node

ğŸ‰ **Data persists across Pod restarts AND is shared by replicas!**

---

# âš ï¸ But: hostPath has a big limitation

`hostPath` is **node-specific**.

In a multi-node cluster:

```
Node 1 /data    â‰    Node 2 /data
```

So:

* If Pod A is on Node 1
* And Pod B is on Node 2

â†’ they **do NOT share the same directory**
â†’ Your data becomes **node-local**, not cluster-wide

This is why `hostPath` is *not used in production* for persistence.

---

# ğŸ§  When is `hostPath` useful?

âœ”ï¸ Local testing (minikube, kind, small clusters)
âœ”ï¸ Development environments
âœ”ï¸ Sharing already existing data from the node
âœ”ï¸ Running daemon tools that need node-local paths (logs, machine files)

---

# âŒ When should you NOT use `hostPath`?

âŒ Production environments
âŒ Multi-node clusters
âŒ When you need cluster-wide persistent data
âŒ When Pods may be rescheduled on other nodes

---

# ğŸ“Œ What comes next?

The next lectures introduce:

### **PersistentVolumes (PV)**

*

### **PersistentVolumeClaims (PVC)**

These solve all remaining problems:

* Storage is **not tied to the Pod**
* Storage is **not tied to the node**
* Storage can be backed by:

  * AWS EBS
  * Azure Disk
  * GCE Persistent Disk
  * NFS
  * Ceph
  * Local storage
  * Cloud-native storage classes

This is the **production-ready solution**.
